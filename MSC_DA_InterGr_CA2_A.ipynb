{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25140be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              #For handling arrays\n",
    "import pandas as pd             # For handling data\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e799531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csv_file = \"file:///home/hduser/Downloads/work2/ProjectTweets.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00511d4d",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1aca407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 44:=============================>                            (3 + 3) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Import pyspark SQL\n",
    "from pyspark.sql import SparkSession        \n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"SparkSQL\")\n",
    "  .getOrCreate())\n",
    "\n",
    "# Read and create a temporary view\n",
    "# The dataset doesnt contain header, so header = false\n",
    "# toDF to define appropriated column name\n",
    "dfTwitter = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .load(csv_file)\n",
    "  .toDF('id', 'seq', 'date', 'query', 'user', 'tweet'))\n",
    "\n",
    "dfTwitter.createOrReplaceTempView(\"tblTempTwitter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2723b7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| id|       seq|                date|   query|           user|               tweet|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM tblTempTwitter\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b064522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-----+----+-----+\n",
      "| id|seq|date|query|user|tweet|\n",
      "+---+---+----+-----+----+-----+\n",
      "+---+---+----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM tblTempTwitter where query != 'NO_QUERY'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b3c0b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 49:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           user|total|\n",
      "+---------------+-----+\n",
      "|       lost_dog|  549|\n",
      "|        webwoke|  345|\n",
      "|       tweetpet|  310|\n",
      "|SallytheShizzle|  281|\n",
      "|    VioletsCRUK|  279|\n",
      "|    mcraddictal|  276|\n",
      "|       tsarnick|  248|\n",
      "|    what_bugs_u|  246|\n",
      "|    Karen230683|  238|\n",
      "|      DarkPiano|  236|\n",
      "|   SongoftheOss|  227|\n",
      "|      Jayme1988|  225|\n",
      "|         keza34|  219|\n",
      "| ramdomthoughts|  216|\n",
      "|      shanajaca|  213|\n",
      "|         wowlew|  212|\n",
      "|   TraceyHewins|  211|\n",
      "|     nuttychris|  211|\n",
      "|   thisgoeshere|  207|\n",
      "|     Spidersamm|  205|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT user, COUNT(user) as total FROM tblTempTwitter GROUP BY user ORDER BY total desc;\"\"\").show(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd34b9",
   "metadata": {},
   "source": [
    "Looking for null or blank date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c37b253e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|tweet|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT user, tweet FROM tblTempTwitter where date is null or date =='';\"\"\").show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d519a7",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343c4e6",
   "metadata": {},
   "source": [
    "### SPARK HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415411a",
   "metadata": {},
   "source": [
    "\n",
    "Creating a Database in Hive Metastore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ff2d7db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 06:55:47,746 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-18 06:55:47,746 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-18 06:55:50,981 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "2023-10-18 06:55:50,982 WARN metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hduser@127.0.1.1\n",
      "2023-10-18 06:55:51,010 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "2023-10-18 06:55:51,392 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "2023-10-18 06:55:51,398 WARN metastore.ObjectStore: Failed to get database dbtwitter, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Database dbTwitter in Hive\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS dbTwitter\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb047c5",
   "metadata": {},
   "source": [
    "Using spark.sql() method \"CREATE TABLE\" to create a table in Hive from the spark temporary view tblTempTwitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "422f3257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 06:57:39,136 WARN analysis.ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "2023-10-18 06:57:39,233 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2023-10-18 06:57:39,361 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "2023-10-18 06:57:39,361 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-18 06:57:39,362 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-18 06:57:39,372 WARN metastore.HiveMetaStore: Location: file:/home/hduser/Downloads/work2/spark-warehouse/dbtwitter.db/tbltwitter specified for non-external table:tbltwitter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Table in Hive tblTwitter on the bdTwitter database.\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dbTwitter.tblTwitter (id Int, seq Double, date String, query String, user String, tweet String)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8dc617",
   "metadata": {},
   "source": [
    "Inserting data from the spark temporary view tblTempTwitter into the Hive table tblTwitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfc7ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert into Hive tblTwitter using the spar temp view tblTempTwitter. \n",
    "spark.sql(\"INSERT INTO TABLE dbTwitter.tblTwitter SELECT * FROM tblTempTwitter\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e35705a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "|    id|          seq|                date|   query|           user|               tweet|\n",
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "|545133|2.201337104E9|Tue Jun 16 20:08:...|NO_QUERY|      alt_ducky|@miss_clariss oh ...|\n",
      "|545134|2.201337108E9|Tue Jun 16 20:08:...|NO_QUERY|     CourtneyVR|Failed my WOF. Wi...|\n",
      "|545135|2.201337287E9|Tue Jun 16 20:08:...|NO_QUERY|    melissaholt|Watching the firs...|\n",
      "|545136|2.201337425E9|Tue Jun 16 20:08:...|NO_QUERY|       itznesha|my computer is in...|\n",
      "|545137|2.201337512E9|Tue Jun 16 20:08:...|NO_QUERY|    lovinmyboys|Worked out my upp...|\n",
      "|545138|2.201337757E9|Tue Jun 16 20:08:...|NO_QUERY|     mikerbrant|OMG I got my new ...|\n",
      "|545139|2.201338077E9|Tue Jun 16 20:08:...|NO_QUERY|         daulex|my back has flare...|\n",
      "|545140|2.201338113E9|Tue Jun 16 20:08:...|NO_QUERY|    CaliHeather|I am starting to ...|\n",
      "|545141|2.201338137E9|Tue Jun 16 20:08:...|NO_QUERY|nessaluvsu2much|@dacialuvsu2much ...|\n",
      "|545142|2.201338439E9|Tue Jun 16 20:08:...|NO_QUERY|       agthekid|*Sees the next gi...|\n",
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets view the data in the hive table\n",
    "spark.sql(\"SELECT * FROM dbTwitter.tblTwitter\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e6b7e",
   "metadata": {},
   "source": [
    "### MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "595deed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkMySQL = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"SparkMySQL\")\n",
    "  .config(\"spark.jars\", \"mysql-connector-java-8.1.0.jar\")\n",
    "  .getOrCreate())\n",
    "\n",
    "#spark = SparkSession.builder \\\n",
    "#  .appName(\"MyApp\") \\\n",
    "#  .config(\"spark.jars\", \"mysql-connector-java-8.0.27.jar\") \\\n",
    "#  .config(\"spark.driver.extraClassPath\", \"mysql-connector-java-8.0.27.jar\") \\\n",
    "#  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a09d4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d1917c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysqlclient\n",
      "  Downloading mysqlclient-2.2.0.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.5/89.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[27 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m /bin/sh: 1: pkg-config: not found\n",
      "  \u001b[31m   \u001b[0m /bin/sh: 1: pkg-config: not found\n",
      "  \u001b[31m   \u001b[0m Trying pkg-config --exists mysqlclient\n",
      "  \u001b[31m   \u001b[0m Command 'pkg-config --exists mysqlclient' returned non-zero exit status 127.\n",
      "  \u001b[31m   \u001b[0m Trying pkg-config --exists mariadb\n",
      "  \u001b[31m   \u001b[0m Command 'pkg-config --exists mariadb' returned non-zero exit status 127.\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/hduser/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/hduser/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/hduser/anaconda3/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-5cwpb8ro/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 355, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-5cwpb8ro/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 325, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-5cwpb8ro/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 341, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 154, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 48, in get_config_posix\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 27, in find_package_name\n",
      "  \u001b[31m   \u001b[0m Exception: Can not find valid pkg-config name.\n",
      "  \u001b[31m   \u001b[0m Specify MYSQLCLIENT_CFLAGS and MYSQLCLIENT_LDFLAGS env vars manually\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysqlclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e69ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cae6b47c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MySQLdb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mMySQLdb\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MySQLdb'"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d30fcc65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mysql'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmysql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\n\u001b[1;32m      3\u001b[0m db_connection \u001b[38;5;241m=\u001b[39m mysql\u001b[38;5;241m.\u001b[39mconnector\u001b[38;5;241m.\u001b[39mconnect(user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhduser\u001b[39m\u001b[38;5;124m\"\u001b[39m, password\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass@word1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m db_cursor \u001b[38;5;241m=\u001b[39m db_connection\u001b[38;5;241m.\u001b[39mcursor()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mysql'"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "db_connection = mysql.connector.connect(user=\"hduser\", password=\"Pass@word1\")\n",
    "db_cursor = db_connection.cursor()\n",
    "db_cursor.execute(\"CREATE DATABASE IF NOT EXISTS dbTwitter;\")\n",
    "db_cursor.execute(\"USE dbTwitter;\")\n",
    "\n",
    "db_cursor.execute(\"CREATE TABLE IF NOT EXISTS dbTwitter.tblTwitter (id Int, seq Double, date String, query String, user String, tweet String)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29267e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTwitter.write \n",
    "  .format(\"jdbc\") \n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \n",
    "  .option(\"url\", jdbcUrl)\n",
    "  .option(\"dbtable\", \"tblTwitter\") \n",
    "  .option(\"user\", username) \n",
    "  .option(\"password\", password)\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32773dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl = \"jdbc:mysql://<host>:<port>/<database>\"\n",
    "username = \"hduser\"\n",
    "password = \"Pass@word1\"\n",
    "\n",
    "df = spark.read \n",
    "  .format(\"jdbc\") \n",
    "  .option(\"url\", jdbcUrl) \n",
    "  .option(\"user\", username) \n",
    "  .option(\"password\", password) \n",
    "  .option(\"dbtable\", \"<table>\") \n",
    "  .option(\"inferSchema\", \"true\") \n",
    "  .load()\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
