{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25140be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              #For handling arrays\n",
    "import pandas as pd             # For handling data\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "#from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e799531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csv_file = \"file:///home/hduser/Downloads/work2/ProjectTweets.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00511d4d",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1aca407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Import pyspark SQL\n",
    "from pyspark.sql import SparkSession        \n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"SparkSQL\")\n",
    "  .getOrCreate())\n",
    "\n",
    "# Read and create a temporary view\n",
    "# The dataset doesnt contain header, so header = false\n",
    "# toDF to define appropriated column name\n",
    "dfTwitter = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .load(csv_file)\n",
    "  .toDF('id', 'seq', 'date', 'query', 'user', 'tweet'))\n",
    "\n",
    "dfTwitter.createOrReplaceTempView(\"tblTempTwitter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2723b7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| id|       seq|                date|   query|           user|               tweet|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM tblTempTwitter\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b064522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-----+----+-----+\n",
      "| id|seq|date|query|user|tweet|\n",
      "+---+---+----+-----+----+-----+\n",
      "+---+---+----+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM tblTempTwitter where query != 'NO_QUERY'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b3c0b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           user|total|\n",
      "+---------------+-----+\n",
      "|       lost_dog|  549|\n",
      "|        webwoke|  345|\n",
      "|       tweetpet|  310|\n",
      "|SallytheShizzle|  281|\n",
      "|    VioletsCRUK|  279|\n",
      "|    mcraddictal|  276|\n",
      "|       tsarnick|  248|\n",
      "|    what_bugs_u|  246|\n",
      "|    Karen230683|  238|\n",
      "|      DarkPiano|  236|\n",
      "|   SongoftheOss|  227|\n",
      "|      Jayme1988|  225|\n",
      "|         keza34|  219|\n",
      "| ramdomthoughts|  216|\n",
      "|      shanajaca|  213|\n",
      "|         wowlew|  212|\n",
      "|     nuttychris|  211|\n",
      "|   TraceyHewins|  211|\n",
      "|   thisgoeshere|  207|\n",
      "|     Spidersamm|  205|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT user, COUNT(user) as total FROM tblTempTwitter GROUP BY user ORDER BY total desc;\"\"\").show(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd34b9",
   "metadata": {},
   "source": [
    "Looking for null or blank date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c37b253e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|tweet|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT user, tweet FROM tblTempTwitter where date is null or date =='';\"\"\").show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d519a7",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343c4e6",
   "metadata": {},
   "source": [
    "### SPARK HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415411a",
   "metadata": {},
   "source": [
    "\n",
    "Creating a Database in Hive Metastore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ff2d7db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 09:50:42,644 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-18 09:50:42,646 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-18 09:50:46,798 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "2023-10-18 09:50:46,798 WARN metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hduser@127.0.1.1\n",
      "2023-10-18 09:50:47,225 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "2023-10-18 09:50:47,242 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database dbtwitter already exists)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:925)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n",
      "\tat com.sun.proxy.$Proxy23.create_database(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:725)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
      "\tat com.sun.proxy.$Proxy24.createDatabase(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:434)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createDatabase$1(HiveClientImpl.scala:347)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:345)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:193)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:193)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:251)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:83)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Database dbTwitter in Hive\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS dbTwitter\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb047c5",
   "metadata": {},
   "source": [
    "Using spark.sql() method \"CREATE TABLE\" to create a table in Hive from the spark temporary view tblTempTwitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "422f3257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 09:50:55,618 WARN analysis.ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "2023-10-18 09:50:56,236 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2023-10-18 09:50:56,471 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "2023-10-18 09:50:56,471 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-18 09:50:56,472 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-18 09:50:56,529 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Table tbltwitter already exists)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1416)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1503)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n",
      "\tat com.sun.proxy.$Proxy23.create_table_with_environment_context(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
      "\tat com.sun.proxy.$Proxy24.createTable(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:555)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:553)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:287)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:376)\n",
      "\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:167)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Table in Hive tblTwitter on the bdTwitter database.\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dbTwitter.tblTwitter (id Int, seq Double, date String, query String, user String, tweet String)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8dc617",
   "metadata": {},
   "source": [
    "Inserting data from the spark temporary view tblTempTwitter into the Hive table tblTwitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfc7ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Insert into Hive tblTwitter using the spar temp view tblTempTwitter. \n",
    "spark.sql(\"INSERT INTO TABLE dbTwitter.tblTwitter SELECT * FROM tblTempTwitter\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e35705a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "|    id|          seq|                date|   query|           user|               tweet|\n",
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "|545133|2.201337104E9|Tue Jun 16 20:08:...|NO_QUERY|      alt_ducky|@miss_clariss oh ...|\n",
      "|545134|2.201337108E9|Tue Jun 16 20:08:...|NO_QUERY|     CourtneyVR|Failed my WOF. Wi...|\n",
      "|545135|2.201337287E9|Tue Jun 16 20:08:...|NO_QUERY|    melissaholt|Watching the firs...|\n",
      "|545136|2.201337425E9|Tue Jun 16 20:08:...|NO_QUERY|       itznesha|my computer is in...|\n",
      "|545137|2.201337512E9|Tue Jun 16 20:08:...|NO_QUERY|    lovinmyboys|Worked out my upp...|\n",
      "|545138|2.201337757E9|Tue Jun 16 20:08:...|NO_QUERY|     mikerbrant|OMG I got my new ...|\n",
      "|545139|2.201338077E9|Tue Jun 16 20:08:...|NO_QUERY|         daulex|my back has flare...|\n",
      "|545140|2.201338113E9|Tue Jun 16 20:08:...|NO_QUERY|    CaliHeather|I am starting to ...|\n",
      "|545141|2.201338137E9|Tue Jun 16 20:08:...|NO_QUERY|nessaluvsu2much|@dacialuvsu2much ...|\n",
      "|545142|2.201338439E9|Tue Jun 16 20:08:...|NO_QUERY|       agthekid|*Sees the next gi...|\n",
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets view the data in the hive table\n",
    "spark.sql(\"SELECT * FROM dbTwitter.tblTwitter\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e6b7e",
   "metadata": {},
   "source": [
    "### MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "595deed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparkMySQL = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"SparkMySQL\")\n",
    "  .config(\"spark.jars\", \"mysql-connector-java-8.1.0.jar\")\n",
    "  .getOrCreate())\n",
    "\n",
    "#spark = SparkSession.builder \\\n",
    "#  .appName(\"MyApp\") \\\n",
    "#  .config(\"spark.jars\", \"mysql-connector-java-8.0.27.jar\") \\\n",
    "#  .config(\"spark.driver.extraClassPath\", \"mysql-connector-java-8.0.27.jar\") \\\n",
    "#  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a09d4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1917c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mysqlclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e69ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cae6b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d30fcc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40f0adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLEASE, RUN THE COMMANDS BELOW ON TERMINAL TO CREATE AND GRANT PERMISSIONS TO user1:\n",
    "\n",
    "#mysql -u root -p\n",
    "\n",
    "#CREATE USER 'user1'@'%%' IDENTIFIED BY 'Pass@word1';\n",
    "#GRANT ALL PRIVILEGES ON * . * TO 'Pass@word1'@'%%';\n",
    "#FLUSH PRIVILEGES;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e25199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_connection = mysql.connector.connect(user=\"user1\", password=\"Pass@word1\")\n",
    "db_cursor = db_connection.cursor()\n",
    "db_cursor.execute(\"CREATE DATABASE IF NOT EXISTS dbTwitter;\")\n",
    "db_cursor.execute(\"USE dbTwitter;\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65a4daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cursor.execute(\"CREATE TABLE IF NOT EXISTS dbTwitter.tblTwitter (id NUMERIC, seq NUMERIC, date VARCHAR(50), query VARCHAR(50), user VARCHAR(50), tweet TEXT)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5be6fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfPandasTwitter = dfTwitter.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "128faefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pymysql module\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "45d02757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "connection = pymysql.connect(host = 'localhost',\n",
    "                             user = 'user1',\n",
    "                             password = 'Pass@word1',\n",
    "                             db = 'dbTwitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d107c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980093f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column list for insertion\n",
    "cols = \",\".join([str(i) for i in dfPandasTwitter.columns.tolist()])\n",
    "\n",
    "# Insert DataFrame recrds one by one.\n",
    "for i, row in dfPandasTwitter.iterrows():\n",
    "    sql = \"INSERT INTO tblTwitter (\" + cols + \") VALUES (\" + \"%s,\" * (len(row) - 1) + \"%s)\"\n",
    "    cursor.execute(sql, tuple(row))\n",
    "\n",
    "    # the connection is not autocommitted by default, so we must commit to save our changes\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99fb5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_cursor.execute('SELECT count(*) as total FROM dbTwitter.tblTwitter')\n",
    "\n",
    "table_rows = db_cursor.fetchall()\n",
    "\n",
    "df = pd.DataFrame(table_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a9702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fe9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3496b1",
   "metadata": {},
   "source": [
    " - a positive sentiment, compound ≥ 0.05.\n",
    " - a negative sentiment, compound ≤ -0.05.\n",
    " - a neutral sentiment, the compound is between ]-0.05, 0.05[\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eccfee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vaderSentimental library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create and initialise an object\n",
    "sentiment = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Sentimental_Analysis = pd.read_json('reddit_comments.json')\n",
    "\n",
    "df_Sentimental_Analysis['compound'] = ''\n",
    "df_Sentimental_Analysis['sentiment'] = ''\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "\n",
    "#Iterate on our dataset to change all the values with the new porcentages\n",
    "for i, row in df_Sentimental_Analysis.iterrows():\n",
    "    \n",
    "    text = row[\"comment\"]\n",
    "    sent = sentiment.polarity_scores(text)\n",
    "    #print(\"Sentiment of \" + str(i) + \":\", sent['compound'])\n",
    "    df_Sentimental_Analysis.at[i,'compound'] = sent['compound']\n",
    "    if float(sent['compound']) >= 0.05:\n",
    "        df_Sentimental_Analysis.at[i,'sentiment'] = 'positive'\n",
    "        positive += 1\n",
    "    elif float(sent['compound']) <= -0.05:\n",
    "        df_Sentimental_Analysis.at[i,'sentiment'] = 'negative'\n",
    "        negative += 1\n",
    "    else:\n",
    "        df_Sentimental_Analysis.at[i,'sentiment'] = 'neutral'\n",
    "        neutral += 1\n",
    "\n",
    "print(\"Total positive feedbacks: \" + str(positive))\n",
    "print(\"Total negative feedbacks: \" + str(negative))\n",
    "print(\"Total neutral feedbacks: \" + str(neutral))\n",
    "\n",
    "#df_Sentimental_Analysis\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "df_Sentimental_Analysis.loc[df_Sentimental_Analysis['sentiment'] == 'positive']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
