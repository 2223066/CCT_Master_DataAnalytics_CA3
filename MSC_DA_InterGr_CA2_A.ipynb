{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25140be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6108c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ipython-sql\n",
    "#pip install mysqlclient\n",
    "#pip install mysql-connector-python\n",
    "#pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b83ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np              #For handling arrays\n",
    "import pandas as pd             # For handling data\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e799531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data set\n",
    "csv_file = \"file:///home/hduser/Downloads/work2/ProjectTweets.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00511d4d",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1aca407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Import pyspark SQL\n",
    "from pyspark.sql import SparkSession        \n",
    "\n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"SparkSQL\")\n",
    "  .getOrCreate())\n",
    "\n",
    "# Read and create a temporary view\n",
    "# The dataset doesnt contain header, so header = false\n",
    "# toDF to define appropriated column name\n",
    "dfTwitter = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"false\")\n",
    "  .load(csv_file)\n",
    "  .toDF('id', 'seq', 'date', 'query', 'user', 'tweet'))\n",
    "\n",
    "dfTwitter.createOrReplaceTempView(\"tblTempTwitter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2723b7b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| id|       seq|                date|   query|           user|               tweet|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM tblTempTwitter\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b064522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+-----+----+-----+\n",
      "| id|seq|date|query|user|tweet|\n",
      "+---+---+----+-----+----+-----+\n",
      "+---+---+----+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM tblTempTwitter where query != 'NO_QUERY'\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b3c0b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           user|total|\n",
      "+---------------+-----+\n",
      "|       lost_dog|  549|\n",
      "|        webwoke|  345|\n",
      "|       tweetpet|  310|\n",
      "|SallytheShizzle|  281|\n",
      "|    VioletsCRUK|  279|\n",
      "|    mcraddictal|  276|\n",
      "|       tsarnick|  248|\n",
      "|    what_bugs_u|  246|\n",
      "|    Karen230683|  238|\n",
      "|      DarkPiano|  236|\n",
      "|   SongoftheOss|  227|\n",
      "|      Jayme1988|  225|\n",
      "|         keza34|  219|\n",
      "| ramdomthoughts|  216|\n",
      "|      shanajaca|  213|\n",
      "|         wowlew|  212|\n",
      "|     nuttychris|  211|\n",
      "|   TraceyHewins|  211|\n",
      "|   thisgoeshere|  207|\n",
      "|     Spidersamm|  205|\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT user, COUNT(user) as total FROM tblTempTwitter GROUP BY user ORDER BY total desc;\"\"\").show(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dd34b9",
   "metadata": {},
   "source": [
    "Looking for null or blank date values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c37b253e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|user|tweet|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT user, tweet FROM tblTempTwitter where date is null or date =='';\"\"\").show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d519a7",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343c4e6",
   "metadata": {},
   "source": [
    "### SPARK HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415411a",
   "metadata": {},
   "source": [
    "\n",
    "Creating a Database in Hive Metastore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ff2d7db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 09:50:42,644 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-18 09:50:42,646 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-18 09:50:46,798 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "2023-10-18 09:50:46,798 WARN metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hduser@127.0.1.1\n",
      "2023-10-18 09:50:47,225 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "2023-10-18 09:50:47,242 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database dbtwitter already exists)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:925)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n",
      "\tat com.sun.proxy.$Proxy23.create_database(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:725)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
      "\tat com.sun.proxy.$Proxy24.createDatabase(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:434)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createDatabase$1(HiveClientImpl.scala:347)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:345)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:193)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:193)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:251)\n",
      "\tat org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:83)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Database dbTwitter in Hive\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS dbTwitter\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb047c5",
   "metadata": {},
   "source": [
    "Using spark.sql() method \"CREATE TABLE\" to create a table in Hive from the spark temporary view tblTempTwitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "422f3257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 09:50:55,618 WARN analysis.ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "2023-10-18 09:50:56,236 WARN session.SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "2023-10-18 09:50:56,471 WARN conf.HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "2023-10-18 09:50:56,471 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "2023-10-18 09:50:56,472 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "2023-10-18 09:50:56,529 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Table tbltwitter already exists)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1416)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1503)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n",
      "\tat com.sun.proxy.$Proxy23.create_table_with_environment_context(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2396)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:93)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:750)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:738)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)\n",
      "\tat com.sun.proxy.$Proxy24.createTable(Unknown Source)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:859)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:874)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createTable$1(HiveClientImpl.scala:555)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:553)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createTable$1(HiveExternalCatalog.scala:287)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:376)\n",
      "\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:167)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a Table in Hive tblTwitter on the bdTwitter database.\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS dbTwitter.tblTwitter (id Int, seq Double, date String, query String, user String, tweet String)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8dc617",
   "metadata": {},
   "source": [
    "Inserting data from the spark temporary view tblTempTwitter into the Hive table tblTwitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfc7ee36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table is not empty!\n"
     ]
    }
   ],
   "source": [
    "query = spark.sql(\"Select * from dbTwitter.tblTwitter LIMIT 1;\")\n",
    "\n",
    "#Test if the table is empty\n",
    "if not query:\n",
    "    #If the table is empty, them execute the code to fill with the dataset data\n",
    "    #Insert into Hive tblTwitter using the spar temp view tblTempTwitter. \n",
    "    spark.sql(\"INSERT INTO TABLE dbTwitter.tblTwitter SELECT * FROM tblTempTwitter\")    \n",
    "else:\n",
    "    print(\"Table is not empty!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e35705a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "|    id|          seq|                date|   query|           user|               tweet|\n",
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "|545133|2.201337104E9|Tue Jun 16 20:08:...|NO_QUERY|      alt_ducky|@miss_clariss oh ...|\n",
      "|545134|2.201337108E9|Tue Jun 16 20:08:...|NO_QUERY|     CourtneyVR|Failed my WOF. Wi...|\n",
      "|545135|2.201337287E9|Tue Jun 16 20:08:...|NO_QUERY|    melissaholt|Watching the firs...|\n",
      "|545136|2.201337425E9|Tue Jun 16 20:08:...|NO_QUERY|       itznesha|my computer is in...|\n",
      "|545137|2.201337512E9|Tue Jun 16 20:08:...|NO_QUERY|    lovinmyboys|Worked out my upp...|\n",
      "|545138|2.201337757E9|Tue Jun 16 20:08:...|NO_QUERY|     mikerbrant|OMG I got my new ...|\n",
      "|545139|2.201338077E9|Tue Jun 16 20:08:...|NO_QUERY|         daulex|my back has flare...|\n",
      "|545140|2.201338113E9|Tue Jun 16 20:08:...|NO_QUERY|    CaliHeather|I am starting to ...|\n",
      "|545141|2.201338137E9|Tue Jun 16 20:08:...|NO_QUERY|nessaluvsu2much|@dacialuvsu2much ...|\n",
      "|545142|2.201338439E9|Tue Jun 16 20:08:...|NO_QUERY|       agthekid|*Sees the next gi...|\n",
      "+------+-------------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets view the data in the hive table\n",
    "spark.sql(\"SELECT * FROM dbTwitter.tblTwitter\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e6b7e",
   "metadata": {},
   "source": [
    "### MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60da764",
   "metadata": {},
   "source": [
    "Please intall the my sql connector version 8.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "595deed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening a new SparkSession to handle with MySQL\n",
    "#sparkMySQL = (SparkSession\n",
    "#  .builder\n",
    "#  .appName(\"SparkMySQL\")\n",
    "#  .config(\"spark.jars\", \"mysql-connector-java-8.1.0.jar\")\n",
    "#  .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb524f66",
   "metadata": {},
   "source": [
    "<b>PLEASE, RUN THE COMMANDS BELOW ON TERMINAL TO CREATE AND GRANT PERMISSIONS TO user1:</b>\n",
    "<br/>\n",
    "<br/>\n",
    "mysql -u root -p\n",
    "<br/>\n",
    "CREATE USER 'user1'@'%%' IDENTIFIED BY 'Pass@word1';\n",
    "<br/>\n",
    "GRANT ALL PRIVILEGES ON * . * TO 'Pass@word1'@'%%';\n",
    "<br/>\n",
    "FLUSH PRIVILEGES;\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77dbd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a connection to the mySQL database \n",
    "connMySQL = pymysql.connect(host = 'localhost',\n",
    "                             user = 'user1',\n",
    "                             password = 'Pass@word1',\n",
    "                             db = 'dbTwitter')\n",
    "\n",
    "cursor = connMySQL.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4bc259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a database connection to my sql\n",
    "#db_connection = mysql.connector.connect(user=\"user1\", password=\"Pass@word1\")\n",
    "#db_cursor = db_connection.cursor()\n",
    "#Creating a new database if not exists\n",
    "#db_cursor.execute(\"CREATE DATABASE IF NOT EXISTS dbTwitter;\")\n",
    "#db_cursor.execute(\"USE dbTwitter;\")\n",
    "\n",
    "#Creating a new table if not exists\n",
    "#db_cursor.execute(\"CREATE TABLE IF NOT EXISTS dbTwitter.tblTwitter (id NUMERIC, seq NUMERIC, date VARCHAR(50), query VARCHAR(50), user VARCHAR(50), tweet TEXT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "663588ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Creating a new database if not exists\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS dbTwitter;\")\n",
    "#Use the dbTwitter in the context\n",
    "cursor.execute(\"USE dbTwitter;\")\n",
    "\n",
    "#Creating a new table if not exists\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS dbTwitter.tblTwitter (id NUMERIC, seq NUMERIC, date VARCHAR(50), query VARCHAR(50), user VARCHAR(50), tweet TEXT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e98bac7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This table is not empty!\n"
     ]
    }
   ],
   "source": [
    "#COPY the data from HIVE table to My SQL table\n",
    "\n",
    "query = connMySQL.query('SELECT id FROM dbTwitter.tblTwitter LIMIT 1;')\n",
    "\n",
    "#Test if the table is empty\n",
    "if not query:\n",
    "    # convert the spark dataframe to pandas dataframe to iterate \n",
    "    dfPandasTwitter = dfTwitter.toPandas()\n",
    "    \n",
    "    #If the table is empty, them execute the code to fill with the dataset data\n",
    "    # reading the columns to use on the insert clause\n",
    "    cols = \",\".join([str(i) for i in dfPandasTwitter.columns.tolist()])\n",
    "\n",
    "    # Insert on the mySQL table all records from SPark SQL\n",
    "    for i, row in dfPandasTwitter.iterrows():\n",
    "        sql = \"INSERT INTO tblTwitter (\" + cols + \") VALUES (\" + \"%s,\" * (len(row) - 1) + \"%s)\"\n",
    "        cursor.execute(sql, tuple(row))\n",
    "        \n",
    "        # the connection is not autocommitted by default, so commit command is necessary to save the changes\n",
    "        connMySQL.commit()\n",
    "else:\n",
    "    print (\"This table is not empty!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425988c",
   "metadata": {},
   "source": [
    "Creating a new table to do the data preparation. \n",
    "<br/>\n",
    "The ideia is to keep the original mysql table to perform performance test against Hive table (both have same structure and amount of data).\n",
    "<br/>\n",
    "It was excluded seq and query features\n",
    "<br/>\n",
    "It was created day, month and year features to help on the preditctive algorithms\n",
    "<br/>\n",
    "It was created sentiment to store the sentiment analysis for each tweet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3101e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the new table to start the data preparation\n",
    "# It was excluded \n",
    "cursor.execute(\"\"\"CREATE TABLE IF NOT EXISTS dbTwitter.tblTwitterP (\n",
    "                id Int, \n",
    "                date VARCHAR(50), \n",
    "                user VARCHAR(50), \n",
    "                tweet TEXT, \n",
    "                week CHAR(3), \n",
    "                hour Int, \n",
    "                minute Int, \n",
    "                second Int, \n",
    "                day Int, \n",
    "                month Int, \n",
    "                year Int, \n",
    "                sentiment Double)\n",
    "              \"\"\")\n",
    "\n",
    "connMySQL.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c23d7347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#cursor.execute(\"DROP TABLE dbTwitter.tblTwitterP;\")\n",
    "#connMySQL.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f651c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "    cursor.execute(\"\"\"INSERT INTO dbTwitter.tblTwitterP (id, date, user, tweet, week, hour, minute, second, day, month, year, sentiment)\n",
    "                  SELECT id, \n",
    "                  date, \n",
    "                  user, \n",
    "                  tweet,\n",
    "                  SUBSTR(date, 1, 3), \n",
    "                  CAST(SUBSTR(date, 12, 2) AS INTEGER), \n",
    "                  CAST(SUBSTR(date, 15, 2) AS INTEGER),                    \n",
    "                  CAST(SUBSTR(date, 18, 2) AS INTEGER),                                      \n",
    "                  CAST(SUBSTR(date, 9, 2) AS INTEGER),                                                        \n",
    "                  CASE SUBSTR(date, 5, 3)                      \n",
    "                      WHEN 'apr' THEN 4\n",
    "                      WHEN 'may' THEN 5\n",
    "                      WHEN 'jun' THEN 6                      \n",
    "                      ELSE NULL\n",
    "                  END,   \n",
    "                  CAST(SUBSTR(date, 25, 4) AS INTEGER),                                                                          \n",
    "                  0.0  \n",
    "                  FROM dbTwitter.tblTwitter;\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6a64cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = connMySQL.query('SELECT id FROM dbTwitter.tblTwitterP LIMIT 1;')\n",
    "\n",
    "#Test if the table is empty\n",
    "if not query:\n",
    "    #If the table is empty, them execute the code to fill with the dataset data\n",
    "    #Insert into Hive tblTwitter using the spar temp view tblTempTwitter. \n",
    "    \n",
    "    cursor.execute(\"\"\"INSERT INTO dbTwitter.tblTwitterP (id, date, user, tweet, week, hour, minute, second, day, month, year, sentiment)\n",
    "                  SELECT id, \n",
    "                  date, \n",
    "                  user, \n",
    "                  tweet,\n",
    "                  SUBSTR(date, 1, 3), \n",
    "                  SUBSTR(date, 12, 2), \n",
    "                  SUBSTR(date, 15, 2),                    \n",
    "                  SUBSTR(date, 18, 2),                                      \n",
    "                  SUBSTR(date, 9, 2),                                                        \n",
    "                  CASE SUBSTR(date, 5, 3)                      \n",
    "                      WHEN 'apr' THEN 4\n",
    "                      WHEN 'may' THEN 5\n",
    "                      WHEN 'jun' THEN 6                      \n",
    "                      ELSE NULL\n",
    "                  END,   \n",
    "                  SUBSTR(date, 25, 4),                                                                          \n",
    "                  0.0  \n",
    "                  FROM dbTwitter.tblTwitter;\"\"\")   \n",
    "    \n",
    "    connMySQL.commit()\n",
    "    \n",
    "else:\n",
    "    print(\"Table is not empty!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f3d9285e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9819</td>\n",
       "      <td>Sat Apr 18 06:53:45 PDT 2009</td>\n",
       "      <td>prempanicker</td>\n",
       "      <td>Ah okay, consensus is this is a tame start. Th...</td>\n",
       "      <td>Sat</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                             1             2   \\\n",
       "0  9819  Sat Apr 18 06:53:45 PDT 2009  prempanicker   \n",
       "\n",
       "                                                  3    4   5   6   7   8   9   \\\n",
       "0  Ah okay, consensus is this is a tame start. Th...  Sat   6  53  45  18   4   \n",
       "\n",
       "     10   11  \n",
       "0  2009  0.0  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#cursor.execute(\"Select SUBSTR(date, 5, 3) from dbTwitter.tblTwitter LIMIT 1; \")\n",
    "cursor.execute(\"SELECT * FROM dbTwitter.tblTwitterP where week = 'sat' LIMIT 1; \")\n",
    "\n",
    "table_rows = cursor.fetchall()\n",
    "\n",
    "df = pd.DataFrame(table_rows)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "41f03047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5355704e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat Apr 18 08:16:31 PDT 2009</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0   1\n",
       "0  Sat Apr 18 08:16:31 PDT 2009  31"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cursor.execute(\"Select SUBSTR(date, 5, 3) from dbTwitter.tblTwitter LIMIT 1; \")\n",
    "cursor.execute(\"\"\"Select date,\n",
    "                SUBSTR(date, 18, 2)\n",
    "                from dbTwitter.tblTwitter \n",
    "                where id = 11000 LIMIT 1;\"\"\")\n",
    "\n",
    "table_rows = cursor.fetchall()\n",
    "\n",
    "df = pd.DataFrame(table_rows)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "30a64a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat Apr 18 08:16:31 PDT 2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0\n",
       "0  Sat Apr 18 08:16:31 PDT 2009"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#cursor.execute(\"Select SUBSTR(date, 5, 3) from dbTwitter.tblTwitter LIMIT 1; \")\n",
    "cursor.execute(\"Select date from dbTwitter.tblTwitter where id = 11000 LIMIT 1;\")\n",
    "\n",
    "table_rows = cursor.fetchall()\n",
    "\n",
    "df = pd.DataFrame(table_rows)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f22fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819694f",
   "metadata": {},
   "source": [
    " - a positive sentiment, compound ≥ 0.05.\n",
    " - a negative sentiment, compound ≤ -0.05.\n",
    " - a neutral sentiment, the compound is between ]-0.05, 0.05[\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62809059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vaderSentimental library\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create and initialise an object\n",
    "sentiment = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e347c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Sentimental_Analysis = pd.read_json('reddit_comments.json')\n",
    "\n",
    "df_Sentimental_Analysis['compound'] = ''\n",
    "df_Sentimental_Analysis['sentiment'] = ''\n",
    "\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "\n",
    "#Iterate on our dataset to change all the values with the new porcentages\n",
    "for i, row in df_Sentimental_Analysis.iterrows():\n",
    "    \n",
    "    text = row[\"comment\"]\n",
    "    sent = sentiment.polarity_scores(text)\n",
    "    #print(\"Sentiment of \" + str(i) + \":\", sent['compound'])\n",
    "    df_Sentimental_Analysis.at[i,'compound'] = sent['compound']\n",
    "    if float(sent['compound']) >= 0.05:\n",
    "        df_Sentimental_Analysis.at[i,'sentiment'] = 'positive'\n",
    "        positive += 1\n",
    "    elif float(sent['compound']) <= -0.05:\n",
    "        df_Sentimental_Analysis.at[i,'sentiment'] = 'negative'\n",
    "        negative += 1\n",
    "    else:\n",
    "        df_Sentimental_Analysis.at[i,'sentiment'] = 'neutral'\n",
    "        neutral += 1\n",
    "\n",
    "print(\"Total positive feedbacks: \" + str(positive))\n",
    "print(\"Total negative feedbacks: \" + str(negative))\n",
    "print(\"Total neutral feedbacks: \" + str(neutral))\n",
    "\n",
    "#df_Sentimental_Analysis\n",
    "\n",
    "pd.options.display.max_colwidth = 1000\n",
    "\n",
    "df_Sentimental_Analysis.loc[df_Sentimental_Analysis['sentiment'] == 'positive']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
